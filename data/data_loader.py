

# """
# LEAK-FREE FinGAT 2025 - INDIAN MARKET VERSION
# âœ… NO data leakage (proper temporal windows)
# âœ… 5-day buffer between features and target
# âœ… Honest 52-60% expected accuracy
# """

# import os
# import pandas as pd
# import numpy as np
# import torch
# from torch_geometric.data import Data
# from torch_geometric.utils import add_self_loops
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.neighbors import NearestNeighbors
# from typing import List, Dict, Tuple, Optional
# import warnings
# warnings.filterwarnings('ignore')


# class FinancialDataCollector:
#     """Load Indian stock data from local CSV files"""
    
#     def __init__(self, csv_folder_path: str, max_stocks: int = 500):
#         self.csv_folder_path = csv_folder_path
#         self.max_stocks = max_stocks
        
#     def collect_stock_data(self) -> Tuple[Dict[str, pd.DataFrame], Dict[str, str]]:
#         """Load stock data from CSV files"""
        
#         if not os.path.exists(self.csv_folder_path):
#             raise FileNotFoundError(f"Folder not found: {self.csv_folder_path}")
        
#         all_data = {}
#         csv_files = [f for f in os.listdir(self.csv_folder_path) if f.endswith('.csv')]
        
#         if not csv_files:
#             raise ValueError(f"No CSV files found in {self.csv_folder_path}")
        
#         csv_files = csv_files[:self.max_stocks]
#         print(f"Loading {len(csv_files)} CSV files...")
        
#         for filename in csv_files:
#             try:
#                 filepath = os.path.join(self.csv_folder_path, filename)
                
#                 base_name = os.path.splitext(filename)[0]
#                 if '_data' in base_name.lower():
#                     ticker = base_name.replace('_data', '').replace('_DATA', '').upper()
#                 else:
#                     ticker = base_name.upper()
                
#                 df = pd.read_csv(filepath)
                
#                 date_col = self._find_column(df, ['date', 'datetime', 'timestamp'])
#                 close_col = self._find_column(df, ['close', 'adj_close', 'adjusted_close'])
#                 open_col = self._find_column(df, ['open'])
#                 high_col = self._find_column(df, ['high'])
#                 low_col = self._find_column(df, ['low'])
#                 volume_col = self._find_column(df, ['volume', 'vol'])
                
#                 if not date_col or not close_col:
#                     print(f"Warning: {ticker} missing required columns")
#                     continue
                
#                 column_mapping = {date_col: 'Date', close_col: 'Close'}
#                 if open_col:
#                     column_mapping[open_col] = 'Open'
#                 if high_col:
#                     column_mapping[high_col] = 'High'
#                 if low_col:
#                     column_mapping[low_col] = 'Low'
#                 if volume_col:
#                     column_mapping[volume_col] = 'Volume'
                
#                 df = df.rename(columns=column_mapping)
#                 df['Date'] = pd.to_datetime(df['Date'])
#                 df = df.sort_values('Date').dropna(subset=['Close'])
                
#                 # âœ… CRITICAL: Need more history now (60+ days instead of 50)
#                 if len(df) < 60:
#                     print(f"Warning: {ticker} insufficient data ({len(df)} rows)")
#                     continue
                
#                 all_data[ticker] = df
#                 print(f"âœ“ Loaded {ticker}: {len(df)} rows")
                
#             except Exception as e:
#                 print(f"Error loading {filename}: {e}")
#                 continue
        
#         if not all_data:
#             raise ValueError("No valid stock data could be loaded")
        
#         sectors = self._create_sector_mapping(list(all_data.keys()))
#         return all_data, sectors
    
#     def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> Optional[str]:
#         for col in df.columns:
#             if col.lower() in [name.lower() for name in possible_names]:
#                 return col
#         return None
    
#     def _create_sector_mapping(self, tickers: List[str]) -> Dict[str, str]:
#         """Indian stock sector mappings"""
        
#         sector_map = {
#             # Banks & Financial Services
#             'HDFCBANK': 'Banks', 'ICICIBANK': 'Banks', 'SBIN': 'Banks',
#             'KOTAKBANK': 'Banks', 'AXISBANK': 'Banks', 'INDUSINDBK': 'Banks',
#             'BANKBARODA': 'Banks', 'PNB': 'Banks', 'UNIONBANK': 'Banks',
#             'CANBK': 'Banks', 'IDFCFIRSTB': 'Banks', 'FEDERALBNK': 'Banks',
#             'RBLBANK': 'Banks', 'BANDHANBNK': 'Banks', 'AUBANK': 'Banks',
            
#             # Finance & NBFCs
#             'BAJFINANCE': 'Finance', 'HDFCLIFE': 'Finance', 'SBILIFE': 'Finance',
#             'BAJAJFINSV': 'Finance', 'ICICIGI': 'Finance', 'HDFCAMC': 'Finance',
#             'MUTHOOTFIN': 'Finance', 'CHOLAFIN': 'Finance', 'LICHSGFIN': 'Finance',
#             'PNBHOUSING': 'Finance', 'RECLTD': 'Finance', 'PFC': 'Finance',
#             'SHRIRAMFIN': 'Finance', 'HUDCO': 'Finance', 'IRFC': 'Finance',
            
#             # IT & Software
#             'TCS': 'Software & IT Services', 'INFY': 'Software & IT Services',
#             'WIPRO': 'Software & IT Services', 'HCLTECH': 'Software & IT Services',
#             'TECHM': 'Software & IT Services', 'LTI': 'Software & IT Services',
#             'MINDTREE': 'Software & IT Services', 'MPHASIS': 'Software & IT Services',
#             'COFORGE': 'Software & IT Services', 'LTTS': 'Software & IT Services',
#             'PERSISTENT': 'Software & IT Services', 'OFSS': 'Software & IT Services',
#             'TATAELXSI': 'Software & IT Services', 'INTELLECT': 'Software & IT Services',
            
#             # FMCG
#             'HINDUNILVR': 'FMCG', 'ITC': 'FMCG', 'NESTLEIND': 'FMCG',
#             'BRITANNIA': 'FMCG', 'DABUR': 'FMCG', 'MARICO': 'FMCG',
#             'GODREJCP': 'FMCG', 'COLPAL': 'FMCG', 'TATACONSUM': 'FMCG',
            
#             # Automobile
#             'MARUTI': 'Automobile & Ancillaries', 'TATAMOTORS': 'Automobile & Ancillaries',
#             'M&M': 'Automobile & Ancillaries', 'BAJAJ-AUTO': 'Automobile & Ancillaries',
#             'HEROMOTOCO': 'Automobile & Ancillaries', 'EICHERMOT': 'Automobile & Ancillaries',
#             'TVSMOTOR': 'Automobile & Ancillaries', 'ASHOKLEY': 'Automobile & Ancillaries',
#             'BALKRISIND': 'Automobile & Ancillaries', 'APOLLOTYRE': 'Automobile & Ancillaries',
#             'MRF': 'Automobile & Ancillaries', 'MOTHERSON': 'Automobile & Ancillaries',
#             'BOSCHLTD': 'Automobile & Ancillaries', 'ESCORTS': 'Automobile & Ancillaries',
            
#             # Pharmaceuticals
#             'SUNPHARMA': 'Healthcare', 'DRREDDY': 'Healthcare', 'CIPLA': 'Healthcare',
#             'DIVISLAB': 'Healthcare', 'BIOCON': 'Healthcare', 'AUROPHARMA': 'Healthcare',
#             'LUPIN': 'Healthcare', 'TORNTPHARM': 'Healthcare', 'ALKEM': 'Healthcare',
#             'APOLLOHOSP': 'Healthcare', 'MAXHEALTH': 'Healthcare', 'FORTIS': 'Healthcare',
            
#             # Oil & Gas
#             'RELIANCE': 'Oil & Gas', 'ONGC': 'Oil & Gas', 'IOC': 'Oil & Gas',
#             'BPCL': 'Oil & Gas', 'HINDPETRO': 'Oil & Gas', 'GAIL': 'Oil & Gas',
#             'PETRONET': 'Oil & Gas', 'OIL': 'Oil & Gas', 'MGL': 'Oil & Gas', 'IGL': 'Oil & Gas',
            
#             # Metals & Mining
#             'TATASTEEL': 'Metals & Mining', 'JSWSTEEL': 'Metals & Mining',
#             'HINDALCO': 'Metals & Mining', 'VEDL': 'Metals & Mining',
#             'COALINDIA': 'Metals & Mining', 'JINDALSTEL': 'Metals & Mining',
#             'SAIL': 'Metals & Mining', 'NMDC': 'Metals & Mining',
            
#             # Cement
#             'ULTRACEMCO': 'Cement & Construction', 'GRASIM': 'Cement & Construction',
#             'AMBUJACEM': 'Cement & Construction', 'ACC': 'Cement & Construction',
#             'SHREECEM': 'Cement & Construction', 'RAMCOCEM': 'Cement & Construction',
            
#             # Power
#             'POWERGRID': 'Power', 'NTPC': 'Power', 'ADANIPOWER': 'Power',
#             'TATAPOWER': 'Power', 'ADANIGREEN': 'Power', 'TORNTPOWER': 'Power',
            
#             # Telecom
#             'BHARTIARTL': 'Communication Services', 'IDEA': 'Communication Services',
#             'TATACOMM': 'Communication Services',
            
#             # Chemicals
#             'UPL': 'Chemicals', 'SRF': 'Chemicals', 'ATUL': 'Chemicals',
#             'PIDILITIND': 'Chemicals', 'DEEPAKNTR': 'Chemicals', 'NAVINFLUOR': 'Chemicals',
#             'AARTI': 'Chemicals', 'AARTIIND': 'Chemicals', 'GRANULES': 'Chemicals',
            
#             # Industrials
#             'LT': 'Industrials', 'SIEMENS': 'Industrials', 'ABB': 'Industrials',
#             'BHARATFORG': 'Industrials', 'CUMMINSIND': 'Industrials',
#             'THERMAX': 'Industrials', 'VOLTAS': 'Industrials', 'HAVELLS': 'Industrials',
            
#             # Consumer Discretionary
#             'TITAN': 'Consumer Discretionary', 'DMART': 'Consumer Discretionary',
#             'TRENT': 'Consumer Discretionary', 'ABFRL': 'Consumer Discretionary',
#             'JUBLFOOD': 'Consumer Discretionary', 'RELAXO': 'Consumer Discretionary',
            
#             # Real Estate
#             'DLF': 'Real Estate', 'GODREJPROP': 'Real Estate', 'OBEROIRLTY': 'Real Estate',
#             'PHOENIXLTD': 'Real Estate', 'PRESTIGE': 'Real Estate',
            
#             # Infrastructure
#             'ADANIPORTS': 'Infrastructure & Logistics', 'CONCOR': 'Infrastructure & Logistics',
#             'IRCTC': 'Infrastructure & Logistics', 'INDIGO': 'Infrastructure & Logistics',
            
#             # Cards/Payments
#             'SBICARD': 'Finance', 'PAYTM': 'Software & IT Services',
            
#             # Paint
#             'ASIANPAINT': 'Industrials', 'BERGER': 'Industrials',
#         }
        
#         sectors = {}
#         sector_counts = {}
        
#         for ticker in tickers:
#             sector = sector_map.get(ticker, 'Other')
#             sectors[ticker] = sector
#             sector_counts[sector] = sector_counts.get(sector, 0) + 1
        
#         print("\nðŸ‡®ðŸ‡³ Indian Stock Market Sector Distribution:")
#         print("=" * 60)
#         for sector, count in sorted(sector_counts.items()):
#             print(f"  {sector}: {count} stocks")
#         print("=" * 60)
        
#         return sectors


# class FinancialFeatureEngineer:
#     """LEAK-FREE feature engineering"""
    
#     def __init__(self):
#         self.scaler = StandardScaler()
#         self.sector_encoder = LabelEncoder()
        
#     def create_technical_features(self, all_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
#         """Create technical indicators"""
        
#         features = {}
        
#         for ticker, df in all_data.items():
#             try:
#                 close_prices = df['Close']
                
#                 features[f'{ticker}_returns'] = close_prices.pct_change()
#                 features[f'{ticker}_log_returns'] = np.log(close_prices / close_prices.shift(1))
#                 features[f'{ticker}_volatility_5'] = features[f'{ticker}_returns'].rolling(5).std()
#                 features[f'{ticker}_volatility_20'] = features[f'{ticker}_returns'].rolling(20).std()
#                 features[f'{ticker}_sma_5'] = close_prices.rolling(5).mean()
#                 features[f'{ticker}_sma_20'] = close_prices.rolling(20).mean()
#                 features[f'{ticker}_sma_50'] = close_prices.rolling(50).mean()
                
#                 delta = close_prices.diff()
#                 gain = (delta.where(delta > 0, 0)).rolling(14).mean()
#                 loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
#                 rs = gain / (loss + 1e-8)
#                 features[f'{ticker}_rsi'] = 100 - (100 / (1 + rs))
                
#                 for period in [5, 10, 20]:
#                     features[f'{ticker}_momentum_{period}'] = close_prices / close_prices.shift(period) - 1
                
#                 if 'Volume' in df.columns:
#                     volume = df['Volume']
#                     features[f'{ticker}_volume_ratio'] = volume / volume.rolling(20).mean()
                
#                 print(f"âœ“ Created features for {ticker}")
                
#             except Exception as e:
#                 print(f"Error creating features for {ticker}: {e}")
#                 continue
        
#         return pd.DataFrame(features)
    
#     def create_graph_features(self, features: pd.DataFrame, sectors: Dict[str, str]) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
#         """
#         âœ… LEAK-FREE: 40-day window with 5-day buffer
#         Features: Days -60 to -11
#         Buffer: Days -10 to -6 (not used)
#         Target: Days -5 to -1
#         """
        
#         tickers = list(set([col.split('_')[0] for col in features.columns]))
#         tickers = [t for t in tickers if t in sectors.keys()]
        
#         node_features = []
#         ticker_to_idx = {ticker: idx for idx, ticker in enumerate(tickers)}
        
#         print("\nðŸš€ LEAK-FREE Feature Extraction:")
#         print("=" * 60)
        
#         for ticker in tickers:
#             ticker_cols = [col for col in features.columns if col.startswith(f'{ticker}_')]
#             ticker_data = features[ticker_cols].ffill().fillna(0)
            
#             if len(ticker_data) > 60:
#                 # âœ… FIXED: Days -60 to -11 (50-day window, ends at -11)
#                 ticker_mean = ticker_data.iloc[-60:-11].mean(axis=0).values
#                 ticker_std = ticker_data.iloc[-60:-11].std(axis=0).values
#                 ticker_features = np.concatenate([ticker_mean, ticker_std])
                
#                 if len(node_features) == 0:
#                     print(f"Example: {ticker}")
#                     print(f"  âœ… Feature window: Days -60 to -11 (50 days)")
#                     print(f"  âœ… Buffer: Days -10 to -6 (5 days, not used)")
#                     print(f"  âœ… Target: Days -5 to -1 (5 days)")
#                     print(f"  âœ… NO OVERLAP - NO LEAKAGE!")
#                     print(f"  Features: {len(ticker_mean)} mean + {len(ticker_std)} std = {len(ticker_features)} total")
#             else:
#                 ticker_features = np.zeros(len(ticker_cols) * 2)
            
#             sector_list = list(sectors.values())
#             if len(set(sector_list)) > 1:
#                 self.sector_encoder.fit(sector_list)
#                 sector_encoded = self.sector_encoder.transform([sectors[ticker]])[0]
#             else:
#                 sector_encoded = 0
            
#             combined_features = np.concatenate([ticker_features, [sector_encoded]])
#             node_features.append(combined_features)
        
#         print("=" * 60)
        
#         node_features = np.array(node_features)
#         if node_features.std() > 0:
#             node_features = self.scaler.fit_transform(node_features)
        
#         node_features = torch.tensor(node_features, dtype=torch.float)
        
#         # Create K-NN graph
#         num_nodes = len(tickers)
#         edges = []
        
#         if num_nodes > 1:
#             k_neighbors = min(15, num_nodes - 1)
#             nbrs = NearestNeighbors(n_neighbors=k_neighbors, algorithm='ball_tree')
#             nbrs.fit(node_features.numpy())
#             distances, indices = nbrs.kneighbors(node_features.numpy())
            
#             for i in range(num_nodes):
#                 for j in indices[i]:
#                     if i != j:
#                         edges.append([i, j])
        
#         if edges:
#             edge_index = torch.tensor(list(set(map(tuple, edges))), dtype=torch.long).t().contiguous()
#             edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)
#         else:
#             edge_index = torch.empty((2, 0), dtype=torch.long)
        
#         print(f"âœ… Graph: {edge_index.size(1)} edges, {node_features.size(1)} features per node")
        
#         return node_features, edge_index, ticker_to_idx


# class FinancialDataset:
#     """LEAK-FREE dataset"""
    
#     def __init__(self, csv_folder_path: str, max_stocks: int = 550):
#         self.csv_folder_path = csv_folder_path
#         self.max_stocks = max_stocks
#         self.collector = FinancialDataCollector(csv_folder_path, max_stocks)
#         self.feature_engineer = FinancialFeatureEngineer()
        
#     def prepare_dataset(self) -> Tuple[Data, Dict]:
#         """Prepare dataset"""
        
#         print("=" * 60)
#         print("ðŸš€ LEAK-FREE FinGAT 2025 - INDIAN MARKET")
#         print("Expected: Honest 52-60% Accuracy")
#         print("=" * 60)
        
#         all_data, sectors = self.collector.collect_stock_data()
        
#         print("\nCreating technical features...")
#         features = self.feature_engineer.create_technical_features(all_data)
        
#         print("\nBuilding LEAK-FREE graph...")
#         node_features, edge_index, ticker_to_idx = self.feature_engineer.create_graph_features(features, sectors)
        
#         print("\nCreating BALANCED targets...")
#         targets = self._create_targets(features, list(ticker_to_idx.keys()))
        
#         data = Data(
#             x=node_features,
#             edge_index=edge_index,
#             y=targets,
#             num_nodes=node_features.size(0)
#         )
        
#         metadata = {
#             'ticker_to_idx': ticker_to_idx,
#             'idx_to_ticker': {v: k for k, v in ticker_to_idx.items()},
#             'sectors': sectors,
#             'feature_names': features.columns.tolist(),
#             'num_features': node_features.size(1),
#             'num_stocks': len(all_data),
#             'num_edges': edge_index.size(1),
#             'sector_distribution': self._get_sector_distribution(sectors)
#         }
        
#         print(f"\nâœ“ Dataset ready:")
#         print(f"  - {len(all_data)} stocks")
#         print(f"  - {node_features.size(1)} features per node")
#         print(f"  - {metadata['num_edges']} edges")
        
#         return data, metadata
    
#     def create_temporal_splits(self, data: Data, metadata: Dict, train_ratio: float = 0.7, val_ratio: float = 0.15) -> Tuple[Data, Data, Data]:
#         """Create splits"""
        
#         num_nodes = data.num_nodes
#         num_train = int(num_nodes * train_ratio)
#         num_val = int(num_nodes * val_ratio)
        
#         torch.manual_seed(42)
#         perm = torch.randperm(num_nodes)
        
#         train_idx = perm[:num_train]
#         val_idx = perm[num_train:num_train + num_val]
#         test_idx = perm[num_train + num_val:]
        
#         train_data = self._create_subgraph(data, train_idx)
#         val_data = self._create_subgraph(data, val_idx)
#         test_data = self._create_subgraph(data, test_idx)
        
#         print(f"\nâœ… Data splits:")
#         print(f"   Train: {train_data.num_nodes} stocks")
#         print(f"   Val:   {val_data.num_nodes} stocks")
#         print(f"   Test:  {test_data.num_nodes} stocks")
        
#         return train_data, val_data, test_data
    
#     def _create_subgraph(self, data: Data, node_idx: torch.Tensor) -> Data:
#         x_sub = data.x[node_idx]
#         y_sub = data.y[node_idx]
        
#         mask = torch.isin(data.edge_index[0], node_idx) & torch.isin(data.edge_index[1], node_idx)
#         edge_index_sub = data.edge_index[:, mask]
        
#         old_to_new = torch.full((data.num_nodes,), -1, dtype=torch.long)
#         old_to_new[node_idx] = torch.arange(len(node_idx))
#         edge_index_sub = old_to_new[edge_index_sub]
        
#         return Data(x=x_sub, edge_index=edge_index_sub, y=y_sub, num_nodes=len(node_idx))
    
#     def _create_targets(self, features: pd.DataFrame, tickers: List[str]) -> torch.Tensor:
#         """
#         âœ… LEAK-FREE target creation
#         Target: Days -5 to -1 (future)
#         Volatility: Days -40 to -11 (past, no overlap)
#         """
        
#         all_future_returns = []
#         ticker_return_map = {}
        
#         for ticker in tickers:
#             return_col = f'{ticker}_returns'
#             if return_col in features.columns:
#                 returns = features[return_col].dropna()
#                 if len(returns) > 60:
#                     # âœ… Target: Days -5 to -1
#                     future_return = returns.iloc[-5:].mean()
#                     all_future_returns.append(future_return)
#                     ticker_return_map[ticker] = future_return
        
#         global_median = np.median(all_future_returns) if all_future_returns else 0.0
        
#         print("\nðŸš€ LEAK-FREE Target Creation:")
#         print("=" * 60)
#         print(f"Global median: {global_median:.6f}")
#         print(f"Target window: Days -5 to -1")
#         print(f"Feature window: Days -60 to -11")
#         print(f"Buffer: 5 days (no overlap)")
        
#         targets = []
#         up_count = 0
#         down_count = 0
        
#         for ticker in tickers:
#             return_col = f'{ticker}_returns'
            
#             if return_col in features.columns:
#                 returns = features[return_col].dropna()
                
#                 if len(returns) > 60:
#                     try:
#                         future_return = ticker_return_map.get(ticker, 0.0)
#                         direction = 1 if future_return > global_median else 0
                        
#                         if direction == 1:
#                             up_count += 1
#                         else:
#                             down_count += 1
                        
#                         # âœ… FIXED: Volatility from Days -60 to -11 (no overlap with target)
#                         vol = returns.iloc[-60:-11].std()
#                         vol_adj_return = future_return / (vol + 1e-8) if not pd.isna(vol) else 0.0
                        
#                         targets.append([future_return, direction, vol_adj_return])
                        
#                     except (IndexError, KeyError):
#                         targets.append([0.0, 0, 0.0])
#                 else:
#                     targets.append([0.0, 0, 0.0])
#             else:
#                 targets.append([0.0, 0, 0.0])
        
#         total = up_count + down_count
#         up_ratio = up_count / total * 100 if total > 0 else 0
        
#         print(f"\nâœ… Target Balance:")
#         print(f"   UP:   {up_count} ({up_ratio:.1f}%)")
#         print(f"   DOWN: {down_count} ({100-up_ratio:.1f}%)")
        
#         if 45 <= up_ratio <= 55:
#             print(f"   âœ… PERFECTLY BALANCED!")
#         elif 40 <= up_ratio <= 60:
#             print(f"   âœ… GOOD BALANCE")
#         else:
#             print(f"   âš ï¸ WARNING: Imbalanced ({up_ratio:.1f}% UP)")
        
#         print("=" * 60)
        
#         return torch.tensor(targets, dtype=torch.float)
    
#     def _get_sector_distribution(self, sectors: Dict[str, str]) -> Dict[str, int]:
#         distribution = {}
#         for sector in sectors.values():
#             distribution[sector] = distribution.get(sector, 0) + 1
#         return distribution


# def main():
#     """Example usage for Indian market data"""
#     csv_folder = "indian_data"
#     dataset = FinancialDataset(csv_folder, max_stocks=550)
    
#     try:
#         graph_data, metadata = dataset.prepare_dataset()
#         train_data, val_data, test_data = dataset.create_temporal_splits(graph_data, metadata)
        
#         print("\n" + "=" * 60)
#         print("âœ… LEAK-FREE DATA READY FOR TRAINING")
#         print("Expected: Honest 52-60% Accuracy")
#         print("=" * 60)
        
#         return graph_data, metadata, train_data, val_data, test_data
        
#     except Exception as e:
#         print(f"Error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None, None, None, None, None


# if __name__ == "__main__":
#     main()





"""
LEAK-FREE FinGAT 2025 - INDIAN MARKET VERSION WITH MULTI-SCALE TEMPORAL FEATURES
âœ… NO data leakage (proper temporal windows)
âœ… 5-day buffer between features and target
âœ… Multi-scale: Short (10d) + Medium (30d) + Long (50d)
âœ… PROPER TEMPORAL SPLITS (by date, not random)
âœ… Expected 55-65% honest accuracy
"""

import os
import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from torch_geometric.utils import add_self_loops
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import NearestNeighbors
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class FinancialDataCollector:
    """Load Indian stock data from local CSV files"""
    
    def __init__(self, csv_folder_path: str, max_stocks: int = 500):
        self.csv_folder_path = csv_folder_path
        self.max_stocks = max_stocks
        
    def collect_stock_data(self) -> Tuple[Dict[str, pd.DataFrame], Dict[str, str]]:
        """Load stock data from CSV files"""
        
        if not os.path.exists(self.csv_folder_path):
            raise FileNotFoundError(f"Folder not found: {self.csv_folder_path}")
        
        all_data = {}
        csv_files = [f for f in os.listdir(self.csv_folder_path) if f.endswith('.csv')]
        
        if not csv_files:
            raise ValueError(f"No CSV files found in {self.csv_folder_path}")
        
        csv_files = csv_files[:self.max_stocks]
        print(f"Loading {len(csv_files)} CSV files...")
        
        for filename in csv_files:
            try:
                filepath = os.path.join(self.csv_folder_path, filename)
                
                base_name = os.path.splitext(filename)[0]
                if '_data' in base_name.lower():
                    ticker = base_name.replace('_data', '').replace('_DATA', '').upper()
                else:
                    ticker = base_name.upper()
                
                df = pd.read_csv(filepath)
                
                date_col = self._find_column(df, ['date', 'datetime', 'timestamp'])
                close_col = self._find_column(df, ['close', 'adj_close', 'adjusted_close'])
                open_col = self._find_column(df, ['open'])
                high_col = self._find_column(df, ['high'])
                low_col = self._find_column(df, ['low'])
                volume_col = self._find_column(df, ['volume', 'vol'])
                
                if not date_col or not close_col:
                    print(f"Warning: {ticker} missing required columns")
                    continue
                
                column_mapping = {date_col: 'Date', close_col: 'Close'}
                if open_col:
                    column_mapping[open_col] = 'Open'
                if high_col:
                    column_mapping[high_col] = 'High'
                if low_col:
                    column_mapping[low_col] = 'Low'
                if volume_col:
                    column_mapping[volume_col] = 'Volume'
                
                df = df.rename(columns=column_mapping)
                df['Date'] = pd.to_datetime(df['Date'])
                df = df.sort_values('Date').dropna(subset=['Close'])
                
                # âœ… CRITICAL: Need more history (60+ days for long-term scale)
                if len(df) < 60:
                    print(f"Warning: {ticker} insufficient data ({len(df)} rows)")
                    continue
                
                all_data[ticker] = df
                print(f"âœ“ Loaded {ticker}: {len(df)} rows")
                
            except Exception as e:
                print(f"Error loading {filename}: {e}")
                continue
        
        if not all_data:
            raise ValueError("No valid stock data could be loaded")
        
        sectors = self._create_sector_mapping(list(all_data.keys()))
        return all_data, sectors
    
    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> Optional[str]:
        for col in df.columns:
            if col.lower() in [name.lower() for name in possible_names]:
                return col
        return None
    
    def _create_sector_mapping(self, tickers: List[str]) -> Dict[str, str]:
        """Indian stock sector mappings"""
        
        sector_map = {
            # Banks & Financial Services
            'HDFCBANK': 'Banks', 'ICICIBANK': 'Banks', 'SBIN': 'Banks',
            'KOTAKBANK': 'Banks', 'AXISBANK': 'Banks', 'INDUSINDBK': 'Banks',
            'BANKBARODA': 'Banks', 'PNB': 'Banks', 'UNIONBANK': 'Banks',
            'CANBK': 'Banks', 'IDFCFIRSTB': 'Banks', 'FEDERALBNK': 'Banks',
            'RBLBANK': 'Banks', 'BANDHANBNK': 'Banks', 'AUBANK': 'Banks',
            'UCOBANK': 'Banks', 'YESBANK': 'Banks',
            
            # Finance & NBFCs
            'BAJFINANCE': 'Finance', 'HDFCLIFE': 'Finance', 'SBILIFE': 'Finance',
            'BAJAJFINSV': 'Finance', 'ICICIGI': 'Finance', 'HDFCAMC': 'Finance',
            'MUTHOOTFIN': 'Finance', 'CHOLAFIN': 'Finance', 'LICHSGFIN': 'Finance',
            'PNBHOUSING': 'Finance', 'RECLTD': 'Finance', 'PFC': 'Finance',
            'SHRIRAMFIN': 'Finance', 'HUDCO': 'Finance', 'IRFC': 'Finance',
            'GICRE': 'Finance', 'POLICYBZR': 'Finance',
            
            # IT & Software
            'TCS': 'Software & IT Services', 'INFY': 'Software & IT Services',
            'WIPRO': 'Software & IT Services', 'HCLTECH': 'Software & IT Services',
            'TECHM': 'Software & IT Services', 'LTI': 'Software & IT Services',
            'MINDTREE': 'Software & IT Services', 'MPHASIS': 'Software & IT Services',
            'COFORGE': 'Software & IT Services', 'LTTS': 'Software & IT Services',
            'PERSISTENT': 'Software & IT Services', 'OFSS': 'Software & IT Services',
            'TATAELXSI': 'Software & IT Services', 'INTELLECT': 'Software & IT Services',
            'HAPPSTMNDS': 'Software & IT Services',
            
            # FMCG
            'HINDUNILVR': 'FMCG', 'ITC': 'FMCG', 'NESTLEIND': 'FMCG',
            'BRITANNIA': 'FMCG', 'DABUR': 'FMCG', 'MARICO': 'FMCG',
            'GODREJCP': 'FMCG', 'COLPAL': 'FMCG', 'TATACONSUM': 'FMCG',
            
            # Automobile
            'MARUTI': 'Automobile & Ancillaries', 'TATAMOTORS': 'Automobile & Ancillaries',
            'M&M': 'Automobile & Ancillaries', 'BAJAJ-AUTO': 'Automobile & Ancillaries',
            'HEROMOTOCO': 'Automobile & Ancillaries', 'EICHERMOT': 'Automobile & Ancillaries',
            'TVSMOTOR': 'Automobile & Ancillaries', 'ASHOKLEY': 'Automobile & Ancillaries',
            'BALKRISIND': 'Automobile & Ancillaries', 'APOLLOTYRE': 'Automobile & Ancillaries',
            'MRF': 'Automobile & Ancillaries', 'MOTHERSON': 'Automobile & Ancillaries',
            'BOSCHLTD': 'Automobile & Ancillaries', 'ESCORTS': 'Automobile & Ancillaries',
            
            # Pharmaceuticals
            'SUNPHARMA': 'Healthcare', 'DRREDDY': 'Healthcare', 'CIPLA': 'Healthcare',
            'DIVISLAB': 'Healthcare', 'BIOCON': 'Healthcare', 'AUROPHARMA': 'Healthcare',
            'LUPIN': 'Healthcare', 'TORNTPHARM': 'Healthcare', 'ALKEM': 'Healthcare',
            'APOLLOHOSP': 'Healthcare', 'MAXHEALTH': 'Healthcare', 'FORTIS': 'Healthcare',
            'PFIZER': 'Healthcare', 'ABBOTINDIA': 'Healthcare',
            
            # Oil & Gas
            'RELIANCE': 'Oil & Gas', 'ONGC': 'Oil & Gas', 'IOC': 'Oil & Gas',
            'BPCL': 'Oil & Gas', 'HINDPETRO': 'Oil & Gas', 'GAIL': 'Oil & Gas',
            'PETRONET': 'Oil & Gas', 'OIL': 'Oil & Gas', 'MGL': 'Oil & Gas', 'IGL': 'Oil & Gas',
            'ADANIENT': 'Oil & Gas',
            
            # Metals & Mining
            'TATASTEEL': 'Metals & Mining', 'JSWSTEEL': 'Metals & Mining',
            'HINDALCO': 'Metals & Mining', 'VEDL': 'Metals & Mining',
            'COALINDIA': 'Metals & Mining', 'JINDALSTEL': 'Metals & Mining',
            'SAIL': 'Metals & Mining', 'NMDC': 'Metals & Mining',
            
            # Cement
            'ULTRACEMCO': 'Cement & Construction', 'GRASIM': 'Cement & Construction',
            'AMBUJACEM': 'Cement & Construction', 'ACC': 'Cement & Construction',
            'SHREECEM': 'Cement & Construction', 'RAMCOCEM': 'Cement & Construction',
            
            # Power
            'POWERGRID': 'Power', 'NTPC': 'Power', 'ADANIPOWER': 'Power',
            'TATAPOWER': 'Power', 'ADANIGREEN': 'Power', 'TORNTPOWER': 'Power',
            
            # Telecom
            'BHARTIARTL': 'Communication Services', 'IDEA': 'Communication Services',
            'TATACOMM': 'Communication Services',
            
            # Chemicals
            'UPL': 'Chemicals', 'SRF': 'Chemicals', 'ATUL': 'Chemicals',
            'PIDILITIND': 'Chemicals', 'DEEPAKNTR': 'Chemicals', 'NAVINFLUOR': 'Chemicals',
            'AARTI': 'Chemicals', 'AARTIIND': 'Chemicals', 'GRANULES': 'Chemicals',
            
            # Industrials
            'LT': 'Industrials', 'SIEMENS': 'Industrials', 'ABB': 'Industrials',
            'BHARATFORG': 'Industrials', 'CUMMINSIND': 'Industrials',
            'THERMAX': 'Industrials', 'VOLTAS': 'Industrials', 'HAVELLS': 'Industrials',
            'ASIANPAINT': 'Industrials', 'BERGER': 'Industrials',
            
            # Consumer Discretionary
            'TITAN': 'Consumer Discretionary', 'DMART': 'Consumer Discretionary',
            'TRENT': 'Consumer Discretionary', 'ABFRL': 'Consumer Discretionary',
            'JUBLFOOD': 'Consumer Discretionary', 'RELAXO': 'Consumer Discretionary',
            'PVRINOX': 'Consumer Discretionary',
            
            # Real Estate
            'DLF': 'Real Estate', 'GODREJPROP': 'Real Estate', 'OBEROIRLTY': 'Real Estate',
            'PHOENIXLTD': 'Real Estate', 'PRESTIGE': 'Real Estate',
            
            # Infrastructure
            'ADANIPORTS': 'Infrastructure & Logistics', 'CONCOR': 'Infrastructure & Logistics',
            'IRCTC': 'Infrastructure & Logistics', 'INDIGO': 'Infrastructure & Logistics',
            
            # Cards/Payments
            'SBICARD': 'Finance', 'PAYTM': 'Software & IT Services',
        }
        
        sectors = {}
        sector_counts = {}
        
        for ticker in tickers:
            sector = sector_map.get(ticker, 'Other')
            sectors[ticker] = sector
            sector_counts[sector] = sector_counts.get(sector, 0) + 1
        
        print("\nðŸ‡®ðŸ‡³ Indian Stock Market Sector Distribution:")
        print("=" * 60)
        for sector, count in sorted(sector_counts.items()):
            print(f"  {sector}: {count} stocks")
        print("=" * 60)
        
        return sectors


class FinancialFeatureEngineer:
    """LEAK-FREE feature engineering with MULTI-SCALE TEMPORAL FEATURES"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.sector_encoder = LabelEncoder()
        
    def create_technical_features(self, all_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """Create technical indicators"""
        
        features = {}
        
        for ticker, df in all_data.items():
            try:
                close_prices = df['Close']
                
                features[f'{ticker}_returns'] = close_prices.pct_change()
                features[f'{ticker}_log_returns'] = np.log(close_prices / close_prices.shift(1))
                features[f'{ticker}_volatility_5'] = features[f'{ticker}_returns'].rolling(5).std()
                features[f'{ticker}_volatility_20'] = features[f'{ticker}_returns'].rolling(20).std()
                features[f'{ticker}_sma_5'] = close_prices.rolling(5).mean()
                features[f'{ticker}_sma_20'] = close_prices.rolling(20).mean()
                features[f'{ticker}_sma_50'] = close_prices.rolling(50).mean()
                
                delta = close_prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
                rs = gain / (loss + 1e-8)
                features[f'{ticker}_rsi'] = 100 - (100 / (1 + rs))
                
                for period in [5, 10, 20]:
                    features[f'{ticker}_momentum_{period}'] = close_prices / close_prices.shift(period) - 1
                
                if 'Volume' in df.columns:
                    volume = df['Volume']
                    features[f'{ticker}_volume_ratio'] = volume / volume.rolling(20).mean()
                
                print(f"âœ“ Created features for {ticker}")
                
            except Exception as e:
                print(f"Error creating features for {ticker}: {e}")
                continue
        
        return pd.DataFrame(features)
    
    def create_graph_features(self, features: pd.DataFrame, sectors: Dict[str, str]) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        """
        âœ… LEAK-FREE MULTI-SCALE TEMPORAL FEATURES:
        - Short-term: 10 days (Days -18 to -9)
        - Medium-term: 30 days (Days -38 to -9)
        - Long-term: 50 days (Days -58 to -9)
        - Buffer: 5 days (Days -8 to -4, not used)
        - Target: Days -3 to -1
        """
        
        tickers = list(set([col.split('_')[0] for col in features.columns]))
        tickers = [t for t in tickers if t in sectors.keys()]
        
        node_features = []
        ticker_to_idx = {ticker: idx for idx, ticker in enumerate(tickers)}
        
        print("\nðŸš€ MULTI-SCALE LEAK-FREE Feature Extraction:")
        print("=" * 60)
        
        first_ticker = True
        
        for ticker in tickers:
            ticker_cols = [col for col in features.columns if col.startswith(f'{ticker}_')]
            ticker_data = features[ticker_cols].ffill().fillna(0)
            
            if len(ticker_data) > 60:
                # âœ… MULTI-SCALE EXTRACTION
                
                # Short-term: Last 10 days (ending at -9)
                short_term_mean = ticker_data.iloc[-18:-8].mean(axis=0).values
                short_term_std = ticker_data.iloc[-18:-8].std(axis=0).values
                
                # Medium-term: Last 30 days (ending at -9)
                medium_term_mean = ticker_data.iloc[-38:-8].mean(axis=0).values
                medium_term_std = ticker_data.iloc[-38:-8].std(axis=0).values
                
                # Long-term: Last 50 days (ending at -9)
                long_term_mean = ticker_data.iloc[-58:-8].mean(axis=0).values
                long_term_std = ticker_data.iloc[-58:-8].std(axis=0).values
                
                # Concatenate all scales: [24 + 24 + 24] = 72 features
                ticker_features = np.concatenate([
                    short_term_mean, short_term_std,
                    medium_term_mean, medium_term_std,
                    long_term_mean, long_term_std
                ])
                
                if first_ticker:
                    print(f"Example: {ticker}")
                    print(f"  âœ… Short-term:  10 days (Days -18 to -9)  â†’ {len(short_term_mean)*2} features")
                    print(f"  âœ… Medium-term: 30 days (Days -38 to -9)  â†’ {len(medium_term_mean)*2} features")
                    print(f"  âœ… Long-term:   50 days (Days -58 to -9)  â†’ {len(long_term_mean)*2} features")
                    print(f"  âœ… Buffer: Days -8 to -4 (5 days, not used)")
                    print(f"  âœ… Target: Days -3 to -1 (3 days)")
                    print(f"  âœ… Total features: {len(ticker_features)} per stock")
                    print(f"  âœ… NO OVERLAP - NO LEAKAGE!")
                    first_ticker = False
            else:
                ticker_features = np.zeros(len(ticker_cols) * 6)  # 6 = 3 scales Ã— 2 (mean+std)
            
            sector_list = list(sectors.values())
            if len(set(sector_list)) > 1:
                self.sector_encoder.fit(sector_list)
                sector_encoded = self.sector_encoder.transform([sectors[ticker]])[0]
            else:
                sector_encoded = 0
            
            combined_features = np.concatenate([ticker_features, [sector_encoded]])
            node_features.append(combined_features)
        
        print("=" * 60)
        
        node_features = np.array(node_features)
        if node_features.std() > 0:
            node_features = self.scaler.fit_transform(node_features)
        
        node_features = torch.tensor(node_features, dtype=torch.float)
        
        # Create K-NN graph
        num_nodes = len(tickers)
        edges = []
        
        if num_nodes > 1:
            k_neighbors = min(15, num_nodes - 1)
            nbrs = NearestNeighbors(n_neighbors=k_neighbors, algorithm='ball_tree')
            nbrs.fit(node_features.numpy())
            distances, indices = nbrs.kneighbors(node_features.numpy())
            
            for i in range(num_nodes):
                for j in indices[i]:
                    if i != j:
                        edges.append([i, j])
        
        if edges:
            edge_index = torch.tensor(list(set(map(tuple, edges))), dtype=torch.long).t().contiguous()
            edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)
        else:
            edge_index = torch.empty((2, 0), dtype=torch.long)
        
        print(f"âœ… Graph: {edge_index.size(1)} edges, {node_features.size(1)} features per node")
        print(f"   Feature breakdown:")
        print(f"   - Short-term (10d): features 0-23")
        print(f"   - Medium-term (30d): features 24-47")
        print(f"   - Long-term (50d): features 48-71")
        print(f"   - Sector encoding: feature 72")
        
        return node_features, edge_index, ticker_to_idx


class FinancialDataset:
    """LEAK-FREE dataset with MULTI-SCALE features and TEMPORAL SPLITTING"""
    
    def __init__(self, csv_folder_path: str, max_stocks: int = 550):
        self.csv_folder_path = csv_folder_path
        self.max_stocks = max_stocks
        self.collector = FinancialDataCollector(csv_folder_path, max_stocks)
        self.feature_engineer = FinancialFeatureEngineer()
        self.all_data = None  # âœ… Store raw data for temporal split
        
    def prepare_dataset(self) -> Tuple[Data, Dict]:
        """Prepare dataset"""
        
        print("=" * 60)
        print("ðŸš€ LEAK-FREE FinGAT 2025 - MULTI-SCALE TEMPORAL")
        print("Expected: 55-65% Honest Accuracy (with proper temporal split)")
        print("=" * 60)
        
        all_data, sectors = self.collector.collect_stock_data()
        self.all_data = all_data  # âœ… Store for temporal split
        
        print("\nCreating technical features...")
        features = self.feature_engineer.create_technical_features(all_data)
        
        print("\nBuilding MULTI-SCALE LEAK-FREE graph...")
        node_features, edge_index, ticker_to_idx = self.feature_engineer.create_graph_features(features, sectors)
        
        print("\nCreating BALANCED targets...")
        targets = self._create_targets(features, list(ticker_to_idx.keys()))
        
        data = Data(
            x=node_features,
            edge_index=edge_index,
            y=targets,
            num_nodes=node_features.size(0)
        )
        
        metadata = {
            'ticker_to_idx': ticker_to_idx,
            'idx_to_ticker': {v: k for k, v in ticker_to_idx.items()},
            'sectors': sectors,
            'feature_names': features.columns.tolist(),
            'num_features': node_features.size(1),
            'num_stocks': len(all_data),
            'num_edges': edge_index.size(1),
            'sector_distribution': self._get_sector_distribution(sectors),
            'feature_scales': {
                'short': (0, 24),
                'medium': (24, 48),
                'long': (48, 72),
                'sector': 72
            }
        }
        
        print(f"\nâœ“ Dataset ready:")
        print(f"  - {len(all_data)} stocks")
        print(f"  - {node_features.size(1)} features per node (73 = 72 temporal + 1 sector)")
        print(f"  - {metadata['num_edges']} edges")
        print(f"  - Multi-scale: Short(10d) + Medium(30d) + Long(50d)")
        
        return data, metadata
    
    def create_temporal_splits(self, data: Data, metadata: Dict, train_ratio: float = 0.7, val_ratio: float = 0.15) -> Tuple[Data, Data, Data]:
        """
        âœ… PROPER TEMPORAL SPLITS USING ACTUAL DATES
        Splits stocks based on their most recent data date
        """
        
        if self.all_data is None:
            raise ValueError("all_data not available. Call prepare_dataset() first.")
        
        # Get last date for each stock
        ticker_dates = []
        for ticker, df in self.all_data.items():
            if ticker in metadata['ticker_to_idx']:
                last_date = df['Date'].iloc[-1]
                ticker_dates.append((ticker, last_date, metadata['ticker_to_idx'][ticker]))
        
        # Sort by date
        ticker_dates_sorted = sorted(ticker_dates, key=lambda x: x[1])
        
        # Split by time
        num_nodes = len(ticker_dates_sorted)
        num_train = int(num_nodes * train_ratio)
        num_val = int(num_nodes * val_ratio)
        
        train_tickers = ticker_dates_sorted[:num_train]
        val_tickers = ticker_dates_sorted[num_train:num_train + num_val]
        test_tickers = ticker_dates_sorted[num_train + num_val:]
        
        # Get indices
        train_idx = torch.tensor([t[2] for t in train_tickers], dtype=torch.long)
        val_idx = torch.tensor([t[2] for t in val_tickers], dtype=torch.long)
        test_idx = torch.tensor([t[2] for t in test_tickers], dtype=torch.long)
        
        train_data = self._create_subgraph(data, train_idx)
        val_data = self._create_subgraph(data, val_idx)
        test_data = self._create_subgraph(data, test_idx)
        
        # Print temporal ranges
        print(f"\nâœ… TEMPORAL Data splits:")
        print(f"   Train: {train_data.num_nodes} stocks")
        print(f"     Date range: {train_tickers[0][1].date()} to {train_tickers[-1][1].date()}")
        print(f"   Val:   {val_data.num_nodes} stocks")
        print(f"     Date range: {val_tickers[0][1].date()} to {val_tickers[-1][1].date()}")
        print(f"   Test:  {test_data.num_nodes} stocks")
        print(f"     Date range: {test_tickers[0][1].date()} to {test_tickers[-1][1].date()}")
        print(f"   âœ… TRULY TEMPORAL: Test stocks have newer data than training!")
        
        return train_data, val_data, test_data
    
    def _create_subgraph(self, data: Data, node_idx: torch.Tensor) -> Data:
        x_sub = data.x[node_idx]
        y_sub = data.y[node_idx]
        
        mask = torch.isin(data.edge_index[0], node_idx) & torch.isin(data.edge_index[1], node_idx)
        edge_index_sub = data.edge_index[:, mask]
        
        old_to_new = torch.full((data.num_nodes,), -1, dtype=torch.long)
        old_to_new[node_idx] = torch.arange(len(node_idx))
        edge_index_sub = old_to_new[edge_index_sub]
        
        return Data(x=x_sub, edge_index=edge_index_sub, y=y_sub, num_nodes=len(node_idx))
    
    def _create_targets(self, features: pd.DataFrame, tickers: List[str]) -> torch.Tensor:
        """
        âœ… LEAK-FREE target creation
        Target: Days -3 to -1 (future)
        Volatility: Days -58 to -9 (past, no overlap)
        """
        
        all_future_returns = []
        ticker_return_map = {}
        
        for ticker in tickers:
            return_col = f'{ticker}_returns'
            if return_col in features.columns:
                returns = features[return_col].dropna()
                if len(returns) > 60:
                    # âœ… Target: Days -3 to -1
                    future_return = returns.iloc[-3:].mean()
                    all_future_returns.append(future_return)
                    ticker_return_map[ticker] = future_return
        
        global_median = np.median(all_future_returns) if all_future_returns else 0.0
        
        print("\nðŸš€ LEAK-FREE Target Creation:")
        print("=" * 60)
        print(f"Global median: {global_median:.6f}")
        print(f"Target window: Days -3 to -1")
        print(f"Feature windows: Short(-18 to -9), Medium(-38 to -9), Long(-58 to -9)")
        print(f"Buffer: Days -8 to -4 (no overlap)")
        
        targets = []
        up_count = 0
        down_count = 0
        
        for ticker in tickers:
            return_col = f'{ticker}_returns'
            
            if return_col in features.columns:
                returns = features[return_col].dropna()
                
                if len(returns) > 60:
                    try:
                        future_return = ticker_return_map.get(ticker, 0.0)
                        direction = 1 if future_return > global_median else 0
                        
                        if direction == 1:
                            up_count += 1
                        else:
                            down_count += 1
                        
                        # âœ… Volatility from Days -58 to -9 (no overlap with target)
                        vol = returns.iloc[-58:-8].std()
                        vol_adj_return = future_return / (vol + 1e-8) if not pd.isna(vol) else 0.0
                        
                        targets.append([future_return, direction, vol_adj_return])
                        
                    except (IndexError, KeyError):
                        targets.append([0.0, 0, 0.0])
                else:
                    targets.append([0.0, 0, 0.0])
            else:
                targets.append([0.0, 0, 0.0])
        
        total = up_count + down_count
        up_ratio = up_count / total * 100 if total > 0 else 0
        
        print(f"\nâœ… Target Balance:")
        print(f"   UP:   {up_count} ({up_ratio:.1f}%)")
        print(f"   DOWN: {down_count} ({100-up_ratio:.1f}%)")
        
        if 45 <= up_ratio <= 55:
            print(f"   âœ… PERFECTLY BALANCED!")
        elif 40 <= up_ratio <= 60:
            print(f"   âœ… GOOD BALANCE")
        else:
            print(f"   âš ï¸ WARNING: Imbalanced ({up_ratio:.1f}% UP)")
        
        print("=" * 60)
        
        return torch.tensor(targets, dtype=torch.float)
    
    def _get_sector_distribution(self, sectors: Dict[str, str]) -> Dict[str, int]:
        distribution = {}
        for sector in sectors.values():
            distribution[sector] = distribution.get(sector, 0) + 1
        return distribution


def main():
    """Example usage for Indian market data"""
    csv_folder = "indian_data"
    dataset = FinancialDataset(csv_folder, max_stocks=550)
    
    try:
        graph_data, metadata = dataset.prepare_dataset()
        train_data, val_data, test_data = dataset.create_temporal_splits(graph_data, metadata)
        
        print("\n" + "=" * 60)
        print("âœ… MULTI-SCALE LEAK-FREE DATA READY FOR TRAINING")
        print("Expected: 55-65% Honest Accuracy (proper temporal validation)")
        print("=" * 60)
        
        return graph_data, metadata, train_data, val_data, test_data
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None


if __name__ == "__main__":
    main()
